{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa08bc6-0717-4c9a-b116-1e4ef1e00e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\u0152835\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\u0152835\\anaconda3\\lib\\site-packages (1.7.4)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\u0152835\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install joblib\n",
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4043583-a842-4545-a7c3-cede7710d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1895\n",
    "end_year = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0901020c-150c-407c-8ff5-9d9777e85096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u0152835\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\u0152835\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.4SP5SUA7CBGXUEOC35YP2ASOICYYEQZZ.gfortran-win_amd64.dll\n",
      "C:\\Users\\u0152835\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fabecb828b42d1b211b6ec86baa6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1895...\n",
      "Reading 1896...\n",
      "Reading 1897...\n",
      "Reading 1898...\n",
      "Reading 1899...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9785a96627a34f88abfe47985b768959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1900...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7950ba0b850e43bfa15202e12436f54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1901...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07203e2670e46a59ca5930e7b14aa92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1902...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7580175a4643a28f8d59d5360050cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1903...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630b2291b74942c0a25c86c5a07ff40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1904...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0678dcd1453485c989f558f22f5df09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1905...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7872fd74600411db2e09012792260a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1906...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b4310a07f846de8b6bef1f3742351e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1907...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\9/ipykernel_28876/2309688002.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\9/ipykernel_28876/2309688002.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myears\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mpapers_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_year_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_vectors_for_year\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m# Add current year data to rolling data and remove the oldest if length exceeds 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\9/ipykernel_28876/2309688002.py\u001b[0m in \u001b[0;36mload_vectors_for_year\u001b[1;34m(year)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Load the entire CSV into a single numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Slice the array to get the desired columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1373\u001b[1;33m     arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n\u001b[0m\u001b[0;32m   1374\u001b[0m                 \u001b[0mconverters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskiplines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m                 \u001b[0munpack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mread_dtype_via_object_chunks\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m             arr = _load_from_filelike(\n\u001b[0m\u001b[0;32m   1017\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m                 \u001b[0mimaginary_unit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimaginary_unit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import faiss\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "path_vectors = '../data/vectors/'\n",
    "CHUNK_SIZE = 1000  # Adjust based on memory availability\n",
    "OUTPUT_PATH = '../data/metrics/papers_cosine.csv'  # Adjust this path as needed\n",
    "N_JOBS = -1  # Use all available cores\n",
    "\n",
    "def load_vectors_for_year(year):\n",
    "    \"\"\"Load vectors for a specific year using efficient reading.\"\"\"\n",
    "    print(f'Reading {year}...')\n",
    "    \n",
    "    file_path = os.path.join(path_vectors, f\"{year}_vectors.csv\")\n",
    "\n",
    "    # Load the entire CSV into a single numpy array\n",
    "    data = np.loadtxt(file_path, delimiter='\\t', dtype=np.float32)\n",
    "\n",
    "    # Slice the array to get the desired columns\n",
    "    papers_ids = data[:, 0].astype(np.int64)  # Assuming the first column is the PaperId\n",
    "    vectors = data[:, 1:]  # Assuming the rest of the columns are the vectors\n",
    "\n",
    "    return papers_ids, vectors\n",
    "\n",
    "def calculate_similarity_for_chunk(chunk, prior_data):\n",
    "    \"\"\"Calculate similarity for a chunk using faiss.\"\"\"\n",
    "    # Ensure data type is float32\n",
    "    chunk = chunk.astype(np.float32)\n",
    "    prior_data = prior_data.astype(np.float32)\n",
    "    \n",
    "    # Normalize the vectors (L2 normalization)\n",
    "    faiss.normalize_L2(chunk)\n",
    "    faiss.normalize_L2(prior_data)\n",
    "    \n",
    "    # Use IndexFlatIP (Inner Product) after normalization for cosine similarity\n",
    "    index = faiss.IndexFlatIP(prior_data.shape[1])\n",
    "    index.add(prior_data)\n",
    "    \n",
    "    # Search for the most similar vector\n",
    "    similarities, _ = index.search(chunk, 1)\n",
    "    \n",
    "    avg_dist = 1 - np.mean(similarities, axis=1)\n",
    "    max_dist = 1 - np.max(similarities, axis=1)\n",
    "    \n",
    "    return avg_dist, max_dist\n",
    "\n",
    "\n",
    "def calculate_avg_max_similarity(current_data, prior_data):\n",
    "    \"\"\"Calculate average and max cosine similarities for chunks.\"\"\"\n",
    "    results = Parallel(n_jobs=N_JOBS)(\n",
    "        delayed(calculate_similarity_for_chunk)(current_data[i:i+CHUNK_SIZE], prior_data)\n",
    "        for i in tqdm(range(0, len(current_data), CHUNK_SIZE))\n",
    "    )\n",
    "    avg_similarities = np.concatenate([res[0] for res in results])\n",
    "    max_similarities = np.concatenate([res[1] for res in results])\n",
    "    return avg_similarities, max_similarities\n",
    "\n",
    "def initialize_output_file():\n",
    "    \"\"\"Initialize the output CSV file with headers.\"\"\"\n",
    "    with open(OUTPUT_PATH, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['PaperId', 'cosine_max', 'cosine_avg'])\n",
    "\n",
    "def save_to_csv(paper_ids, avg_similarities, max_similarities):\n",
    "    \"\"\"Append results to CSV.\"\"\"\n",
    "    with open(OUTPUT_PATH, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for paper_id, avg_sim, max_sim in zip(paper_ids, avg_similarities, max_similarities):\n",
    "            writer.writerow([paper_id, max_sim, avg_sim])\n",
    "\n",
    "def main():\n",
    "    rolling_data = []\n",
    "    years = range(start_year, end_year)\n",
    "\n",
    "    # Initialize the output CSV file\n",
    "    initialize_output_file()\n",
    "\n",
    "    for year in tqdm(years):\n",
    "        papers_ids, current_year_data = load_vectors_for_year(year)\n",
    "        \n",
    "        # Add current year data to rolling data and remove the oldest if length exceeds 5\n",
    "        rolling_data.append(current_year_data)\n",
    "        if len(rolling_data) > 5:\n",
    "            rolling_data.pop(0)\n",
    "        \n",
    "        # If there's not enough prior data, skip the calculations for this year\n",
    "        if len(rolling_data) < 5:\n",
    "            continue\n",
    "        \n",
    "        # Combine prior 5 years data\n",
    "        prior_data = np.vstack(rolling_data[:-1])\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        avg_year_similarities, max_year_similarities = calculate_avg_max_similarity(current_year_data, prior_data)\n",
    "        \n",
    "        # Save results to CSV\n",
    "        save_to_csv(papers_ids, avg_year_similarities, max_year_similarities)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4668f6-6ed4-4a42-b475-9d16eb854e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765ca036dca94fe0a89f1f082ac115b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calculate_avg_max_similarity(current_year_data, prior_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
